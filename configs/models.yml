# Model configuration for AI adapter routing
models:
  # Planning models - for high-level task decomposition
  planner:
    primary:
      provider: anthropic
      model: claude-3-5-sonnet-20241022  # Sonnet 3.5 (latest)
      temperature: 0.7
      max_tokens: 4096
      description: "Primary planner for task decomposition and strategy"
      
    fallback:
      - provider: openai
        model: gpt-4-turbo-preview
        temperature: 0.7
        max_tokens: 4096
        description: "First fallback planner"
        
      - provider: ollama
        model: mixtral:8x7b
        temperature: 0.7
        description: "Local fallback planner"
        
  # Execution models - for task implementation
  exec:
    primary:
      provider: anthropic  
      model: claude-3-opus-20240229  # Opus 3 (most capable)
      temperature: 0.3
      max_tokens: 8192
      description: "Primary executor for complex tasks"
      
    fallback:
      - provider: openai
        model: gpt-4-turbo-preview
        temperature: 0.3
        max_tokens: 8192
        description: "First fallback executor"
        
      - provider: anthropic
        model: claude-3-haiku-20240307  # Haiku for simpler tasks
        temperature: 0.3
        max_tokens: 4096
        description: "Fast fallback executor"
        
      - provider: ollama
        model: codellama:13b
        temperature: 0.3
        description: "Local code-focused fallback"

  # Specialized models for specific tasks
  specialized:
    code_generation:
      provider: openai
      model: gpt-4-turbo-preview
      temperature: 0.2
      max_tokens: 8192
      system_prompt: "You are an expert programmer. Generate clean, efficient code."
      
    text_analysis:
      provider: anthropic
      model: claude-3-haiku-20240307
      temperature: 0.3
      max_tokens: 2048
      
    creative_writing:
      provider: anthropic
      model: claude-3-sonnet-20240229
      temperature: 0.9
      max_tokens: 4096
      
    translation:
      provider: openai
      model: gpt-3.5-turbo
      temperature: 0.3
      max_tokens: 2048

# Routing policies
routing:
  # Policy for selecting models based on task characteristics
  policies:
    complexity_based:
      high:
        - exec.primary
        - planner.primary
      medium:
        - exec.fallback[0]
        - planner.fallback[0]
      low:
        - exec.fallback[1]
        - planner.fallback[1]
        
    cost_optimized:
      priority:
        - planner.fallback[2]  # Ollama (free)
        - exec.fallback[3]     # Ollama (free)
        - exec.fallback[1]     # Haiku (cheap)
        - planner.fallback[0]  # GPT-4 (moderate)
        - planner.primary      # Sonnet (expensive)
        - exec.primary         # Opus (most expensive)
        
    speed_optimized:
      priority:
        - exec.fallback[1]     # Haiku (fastest)
        - planner.fallback[2]  # Local Ollama
        - exec.fallback[3]     # Local Ollama
        - planner.fallback[0]  # GPT-4
        - planner.primary      # Sonnet
        - exec.primary         # Opus

  # Default policy
  default: complexity_based
  
  # Retry configuration
  retry:
    max_attempts: 3
    backoff_factor: 2
    initial_delay: 1.0  # seconds
    
  # Timeout configuration (seconds)
  timeouts:
    planner: 30
    exec: 60
    specialized: 45
    
# Provider configuration
providers:
  anthropic:
    api_key_env: ANTHROPIC_API_KEY
    base_url: https://api.anthropic.com/v1
    rate_limit:
      requests_per_minute: 50
      tokens_per_minute: 100000
      
  openai:
    api_key_env: OPENAI_API_KEY
    base_url: https://api.openai.com/v1
    rate_limit:
      requests_per_minute: 60
      tokens_per_minute: 150000
      
  ollama:
    base_url: http://localhost:11434
    models_available:
      - llama2
      - mixtral:8x7b
      - codellama:13b
      - mistral
    auto_pull: true  # Automatically download models if not present

# Feature flags
features:
  enable_tool_calling: true
  enable_streaming: false
  enable_caching: true
  cache_ttl: 3600  # seconds
  enable_telemetry: true
  log_prompts: false  # Set to true for debugging